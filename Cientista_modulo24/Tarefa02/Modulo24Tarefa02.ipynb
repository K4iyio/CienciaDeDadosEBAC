{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938fd0cd",
   "metadata": {},
   "source": [
    "## Diferenças entre AdaBoost e GBM:\n",
    "\n",
    "1. **Método de Ponderação de Instâncias**:\n",
    "   - **AdaBoost**: Atribui pesos diferentes às instâncias de treinamento, focando mais nas instâncias mal classificadas.\n",
    "   - **GBM**: Usa gradient descent (descida do gradiente) para ajustar os erros residuais das previsões anteriores, não usando pesos de instância.\n",
    "\n",
    "\n",
    "2. **Modelos Fracos**:\n",
    "   - **AdaBoost**: Usa modelos fracos simples, como árvores de decisão rasas (stumps) por padrão.\n",
    "   - **GBM**: Geralmente, utiliza árvores de decisão profundas como modelos fracos, que podem ser mais complexos.\n",
    "\n",
    "\n",
    "3. **Combinação de Modelos**:\n",
    "   - **AdaBoost**: Combina modelos de maneira sequencial, onde cada modelo fraco é ajustado com base nos erros do modelo anterior.\n",
    "   - **GBM**: Combina modelos de maneira sequencial, ajustando cada modelo fraco para minimizar o gradiente da função de perda em relação aos erros residuais.\n",
    "   \n",
    "   \n",
    "4. **Taxa de Aprendizado (Learning Rate)**:\n",
    "   - **AdaBoost**: Possui uma taxa de aprendizado (learning rate) que controla o peso das previsões de cada modelo fraco.\n",
    "   - **GBM**: Também possui uma taxa de aprendizado, que controla a contribuição de cada modelo fraco, mas é usado de maneira diferente do AdaBoost.\n",
    "\n",
    "\n",
    "5. **Algoritmo de Construção de Árvores**:\n",
    "    - **AdaBoost**: Usa árvores de decisão simples, como stumps (árvores com apenas um nó de decisão).\n",
    "    - **GBM**: Utiliza uma técnica de crescimento de árvores chamada \"leaf-wise\" que pode ser mais eficiente em termos de tempo de treinamento e pode resultar em árvores mais profundas e complexas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d976a9",
   "metadata": {},
   "source": [
    "### Exemplo GradientBoostClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98aeea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63d648b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ada128",
   "metadata": {},
   "source": [
    "### Exemplo GradientBoostRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26d3f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c36f2588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-61.05212593]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4233836905173889"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_regression(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "reg.fit(X_train, y_train)\n",
    "print(reg.predict(X_test[1:2]))\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da81d0e3",
   "metadata": {},
   "source": [
    "### Hyperparametros GBM:\n",
    "\n",
    "1. **Número de Árvores (n_estimators)**: Quantas árvores pequenas o modelo deve usar.\n",
    "2. **Profundidade Máxima da Árvore (max_depth)**: Quão profunda cada árvore pode crescer.\n",
    "3. **Taxa de Aprendizado (learning_rate)**: Quão rápido o modelo aprende com os erros.\n",
    "4. **Número Mínimo de Amostras nas Folhas (min_samples_leaf)**: Quanto dados são necessários em uma folha das árvores.\n",
    "5. **Subamostragem (subsample)**: Que fração dos dados é usada para treinar cada árvore.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
